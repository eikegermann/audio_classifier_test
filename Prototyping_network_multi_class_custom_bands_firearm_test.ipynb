{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1MNRR3O-u1uPxeir9GznHqS-QsP2ZFVDC",
      "authorship_tag": "ABX9TyMXsLLM3HGjhDwsq/mjgbdG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eikegermann/audio_classifier_test/blob/main/Prototyping_network_multi_class_custom_bands_firearm_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MArIhg3hN8BZ",
        "outputId": "eeacf5f3-1160-4bb1-a150-e4c1d04b0c28"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load libraries"
      ],
      "metadata": {
        "id": "fa3JAxDiu0hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "import math\n",
        "import random\n",
        "import librosa\n",
        "import librosa.display\n",
        "import pywt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import IPython.display as ipd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from IPython.display import Audio, display\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "r1Ro9HTOFH6e"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define base functions to load dataset\n",
        "Here we establish functions to process the data to be used with the model.\n",
        "The functions are set up to be also usable on individual data samples so the processing for the training data sets and test samples is identical."
      ],
      "metadata": {
        "id": "h9hwPJl-T6ZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Base data functions\n",
        "\n",
        "def load_audio_file(file_path, sr, duration):\n",
        "    audio, _ = librosa.load(file_path, sr=sr, duration=duration, res_type='kaiser_fast')\n",
        "    if len(audio) < sr * duration:\n",
        "        audio = np.pad(audio, (0, sr * duration - len(audio)), mode='constant')\n",
        "    return audio\n",
        "\n",
        "\n",
        "def load_audio_files(data_folder, sr, duration):\n",
        "    class_folders = [subfolder for subfolder in os.listdir(data_folder) if os.path.isdir(os.path.join(data_folder, subfolder))]\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    for class_label, class_folder in enumerate(class_folders):\n",
        "        class_path = os.path.join(data_folder, class_folder)\n",
        "        audio_files = [file for file in os.listdir(class_path) if file.endswith('.wav')]\n",
        "        for audio_file in audio_files:\n",
        "            file_path = os.path.join(class_path, audio_file)\n",
        "            audio = load_audio_file(file_path, sr, duration)\n",
        "            data.append(audio)\n",
        "            labels.append(class_label)\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "\n",
        "def pad_or_truncate(tensor, fixed_length):\n",
        "    if tensor.size(-1) < fixed_length:\n",
        "        return torch.nn.functional.pad(tensor, (0, fixed_length - tensor.size(-1)))\n",
        "    else:\n",
        "        return tensor[..., :fixed_length]\n",
        "\n",
        "\n",
        "## Mel-spectrograms\n",
        "\n",
        "def generate_mel_spectrogram(audio, sr, n_fft, n_mels, fixed_length):\n",
        "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=n_fft, n_mels=n_mels)\n",
        "    mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "    mel_spec = torch.FloatTensor(mel_spec).unsqueeze(0)\n",
        "    mel_spec = pad_or_truncate(mel_spec, fixed_length)\n",
        "    return mel_spec\n",
        "\n",
        "\n",
        "def generate_mel_spectrograms(data, sr, n_fft, n_mels, fixed_length):\n",
        "    mel_spectrograms = []\n",
        "    for audio in data:\n",
        "        mel_spec = generate_mel_spectrogram(audio, sr, n_fft, n_mels, fixed_length)\n",
        "        mel_spectrograms.append(mel_spec)\n",
        "    return torch.stack(mel_spectrograms)\n",
        "\n",
        "\n",
        "## General STFT-spectrograms\n",
        "##### TODO: Use normalisation for network, db for display\n",
        "##### TODO: Try to include phase information by not squaring values\n",
        "\n",
        "def generate_spectrogram(audio, sr, n_fft, fixed_length):\n",
        "    spec = librosa.stft(y=audio, n_fft=n_fft)\n",
        "    spec = np.abs(spec) ** 2\n",
        "    spec = librosa.power_to_db(spec, ref=np.max)\n",
        "    spec = torch.FloatTensor(spec).unsqueeze(0)\n",
        "    spec = pad_or_truncate(spec, fixed_length)\n",
        "    return spec\n",
        "\n",
        "\n",
        "def generate_spectrograms(data, sr, n_fft, fixed_length):\n",
        "    spectrograms = []\n",
        "    for audio in data:\n",
        "        spec = generate_spectrogram(audio, sr, n_fft, fixed_length)\n",
        "        spectrograms.append(spec)\n",
        "    return torch.stack(spectrograms)\n",
        "\n",
        "\n",
        "## Custom spectrograms\n",
        "\n",
        "def number_of_filterbands(fmin, fmax, fraction):\n",
        "    n = 1 + np.ceil(np.log2((fmax/fmin)**(1/fraction)))\n",
        "    return int(n)\n",
        "\n",
        "\n",
        "def create_custom_filterbank(sr, n_fft, fraction, fmin, fmax):\n",
        "    # Calculate the center frequencies for the given fractional octave band\n",
        "    num_bands = number_of_filterbands(fmin, fmax, fraction)\n",
        "    center_frequencies_hz = np.geomspace(fmin, fmax, num=num_bands)\n",
        "\n",
        "    # Calculate the bandwidths for each filter\n",
        "    bandwidths_hz = center_frequencies_hz[1:] - center_frequencies_hz[:-1]\n",
        "\n",
        "    # Calculate corresponding FFT bins\n",
        "    fft_bins = np.floor((n_fft + 1) * center_frequencies_hz / sr).astype(int)\n",
        "\n",
        "    # Create the filterbank matrix\n",
        "    filterbank = np.zeros((len(center_frequencies_hz), int(n_fft // 2 + 1)))\n",
        "\n",
        "    # Construct filters\n",
        "    for i in range(len(center_frequencies_hz) - 1):\n",
        "        center = fft_bins[i]\n",
        "        half_bandwidth = int(round(bandwidths_hz[i] * n_fft / sr))\n",
        "\n",
        "        left = center - half_bandwidth\n",
        "        right = center + half_bandwidth\n",
        "\n",
        "        filterbank[i, left:center] = (np.arange(left, center) - left) / (center - left)\n",
        "        filterbank[i, center:right] = (right - np.arange(center, right)) / (right - center)\n",
        "\n",
        "    return filterbank\n",
        "\n",
        "def generate_custom_spectrogram(audio, filterbank, sr, n_fft, fixed_length):\n",
        "    spec = librosa.stft(y=audio, n_fft=n_fft)\n",
        "    spec = np.abs(spec) ** 2\n",
        "    spec = librosa.power_to_db(spec, ref=np.max)\n",
        "    spec = np.dot(filterbank, spec)\n",
        "    spec = torch.FloatTensor(spec).unsqueeze(0)\n",
        "    spec = pad_or_truncate(spec, fixed_length)\n",
        "    return spec\n",
        "\n",
        "\n",
        "def generate_custom_spectrograms(data, filterbank, sr, n_fft, fixed_length):\n",
        "    spectrograms = []\n",
        "    for audio in data:\n",
        "        spec = generate_custom_spectrogram(audio, filterbank,\n",
        "                                           sr, n_fft, fixed_length)\n",
        "        spectrograms.append(spec)\n",
        "    return torch.stack(spectrograms)\n",
        "\n",
        "\n",
        "\n",
        "## Wavelet spectrograms\n",
        "\n",
        "def generate_cwt_scalogram(audio, sr, scales, fixed_length):\n",
        "    coef, _ = pywt.cwt(audio, scales, 'morl', 1 / sr)\n",
        "    coef = np.abs(coef) ** 2\n",
        "    coef = 10 * np.log10(coef + np.finfo(float).eps)\n",
        "    coef = torch.FloatTensor(coef).unsqueeze(0)\n",
        "    coef = pad_or_truncate(coef, fixed_length)\n",
        "    return coef\n",
        "\n",
        "def generate_cwt_scalograms(data, sr, scales, fixed_length):\n",
        "    scalograms = []\n",
        "    for audio in data:\n",
        "        scalogram = generate_cwt_scalogram(audio, sr, scales, fixed_length)\n",
        "        scalograms.append(scalogram)\n",
        "    return torch.stack(scalograms)\n",
        "\n",
        "def create_scales(f_min, f_max, voices_per_octave, sr):\n",
        "    num_octaves = np.log2(f_max / f_min)\n",
        "    num_scales = int(num_octaves * voices_per_octave)\n",
        "    return np.logspace(np.log2(f_min * sr), np.log2(f_max * sr), num_scales, base=2)\n"
      ],
      "metadata": {
        "id": "nQnsEp-JFLRV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_sample_path = 'drive/MyDrive/audio_ml_data/firearm_samples/test/'\n",
        "\n",
        "# # Load test sample\n",
        "# test_folders = [subfolder for subfolder in os.listdir(test_sample_path) if os.path.isdir(os.path.join(test_sample_path, subfolder))]\n",
        "# test_sample_folder = random.choice(test_folders)\n",
        "# test_sample_path = os.path.join(test_sample_path, test_sample_folder)\n",
        "# test_samples = [file for file in os.listdir(test_sample_path) if file.endswith('.wav')]\n",
        "# test_sample_file = random.choice(test_samples)\n",
        "# test_sample_path = os.path.join(test_sample_path, test_sample_file)\n",
        "\n",
        "# sr = 44100\n",
        "# duration = 1\n",
        "# n_samples = sr * duration\n",
        "\n",
        "# # Parameters for spectrograms\n",
        "# n_fft = int(4096 * 1)\n",
        "# hop_length = n_fft // 4\n",
        "# n_frames = librosa.time_to_frames(duration,\n",
        "#                                   sr=sr,\n",
        "#                                   hop_length=hop_length,\n",
        "#                                   n_fft=n_fft)\n",
        "\n",
        "# # Parameters for custom spectrograms\n",
        "# fmin = 20\n",
        "# fmax = int(sr / 2)\n",
        "# fraction = 1/10\n",
        "# num_filters = number_of_filterbands(fmin, fmax, fraction)\n",
        "# filterbank = create_custom_filterbank(sr, n_fft, fraction, fmin, fmax)\n",
        "\n",
        "# audio = load_audio_file(test_sample_path, sr, duration)\n",
        "# spec = generate_custom_spectrogram(audio, filterbank, sr, n_fft, n_frames)\n",
        "\n",
        "# # Plot the spectrogram\n",
        "# fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "# img = librosa.display.specshow(spec.squeeze().numpy(), x_axis='time', y_axis='mel', sr=sr, fmax=sr // 2)\n",
        "# ax.set_title(\"Mel Spectrogram\")\n",
        "# fig.colorbar(img, ax=ax[1], format=\"%+2.f dB\")\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "Gb_ilMHb3F1v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define model architecture\n",
        "Here, the model architecture is defined. For the case at hand, we are using a small convolutional neural network with a few fully connected layers at the end."
      ],
      "metadata": {
        "id": "iYcUQ1BcUTOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioClassifier(nn.Module):\n",
        "    def __init__(self, n_bands=96, n_frames=97, n_features=3):\n",
        "        super(AudioClassifier, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(p=0.4)\n",
        "\n",
        "        self.fc1 = None\n",
        "        self.fc2 = nn.Linear(2560, 1280)\n",
        "        self.fc2a = nn.Linear(1280, 768)\n",
        "        self.fc2b = nn.Linear(768, 512)\n",
        "        self.fc2c = nn.Linear(512, 256)\n",
        "        self.fc2d = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, n_features)\n",
        "        \n",
        "        self._initialize_fc1(n_bands, n_frames)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc2a(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2b(x))\n",
        "        x = F.relu(self.fc2c(x))\n",
        "        x = F.relu(self.fc2d(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "\n",
        "        return x\n",
        "    \n",
        "    def _initialize_fc1(self, n_bands, n_frames):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sample_input = torch.randn(1, 1, n_bands, n_frames)\n",
        "            # print(\"Training Sample Input Shape:\", sample_input.shape)\n",
        "            x = self.pool(F.relu(self.bn1(self.conv1(sample_input))))\n",
        "            x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "            x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "            # print(\"Training X Shape:\", x.shape)\n",
        "            flattened_size = x.view(x.size(0), -1).shape[1]\n",
        "            # print(flattened_size)\n",
        "            self.fc1 = nn.Linear(flattened_size, 2560)"
      ],
      "metadata": {
        "id": "QH2iMkr_FfVJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Prototype network\n",
        "\n",
        "The network consists of a function that uses the neural network to extract features from the"
      ],
      "metadata": {
        "id": "2ROsN299ViKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run multiple training runs to find best checkpoint\n",
        "\n",
        "# Parameters\n",
        "num_runs = 1\n",
        "\n",
        "sr = 44100\n",
        "duration = 1\n",
        "n_samples = sr * duration\n",
        "\n",
        "# Parameters for spectrograms\n",
        "n_fft = int(4096 * 1)\n",
        "hop_length = n_fft // 4\n",
        "n_frames = librosa.time_to_frames(duration,\n",
        "                                  sr=sr,\n",
        "                                  hop_length=hop_length,\n",
        "                                  n_fft=n_fft)\n",
        "\n",
        "# Parameters for custom spectrograms\n",
        "fmin = 20\n",
        "fmax = int(sr / 2)\n",
        "fraction = 1/10\n",
        "num_filters = number_of_filterbands(fmin, fmax, fraction)\n",
        "filterbank = create_custom_filterbank(sr, n_fft, fraction, fmin, fmax)\n",
        "\n",
        "\n",
        "# Special parameters for mel-spectrograms\n",
        "n_mels = 128\n",
        "\n",
        "# # Special parameters for wavelet spectrograms (scalograms)\n",
        "# lower_bound = 20\n",
        "# upper_bound = 20000\n",
        "# num_voices = 12\n",
        "# scales = create_scales(lower_bound, upper_bound, num_voices, sr)\n",
        "# fixed_length = 128\n",
        "\n",
        "#### Set up training parameters\n",
        "\n",
        "# Training data\n",
        "data_folder = 'drive/MyDrive/audio_ml_data/firearm_samples/train/'\n",
        "num_data_folders = len([subfolder for subfolder in os.listdir(data_folder) if os.path.isdir(os.path.join(data_folder, subfolder))])\n",
        "\n",
        "\n",
        "num_bands = int(n_fft / 2 + 1)\n",
        "fixed_length = n_frames\n",
        "fraction_for_train = 0.7\n",
        "num_episodes = 50\n",
        "num_selected_classes_train = int(num_data_folders * fraction_for_train)\n",
        "display_interval = 25\n",
        "n_features = 24\n",
        "learning_rate = 3e-4\n",
        "weight_decay = 0.0005\n",
        "support_ratio_train = 0.6\n",
        "\n",
        "#### Set up evaluation parameters\n",
        "\n",
        "# Test data\n",
        "eval_data_dir = 'drive/MyDrive/audio_ml_data/firearm_samples/test/'\n",
        "\n",
        "fraction_for_eval = 1\n",
        "num_eval_episodes = 25\n",
        "num_selected_classes_eval = int(num_data_folders * fraction_for_eval)\n",
        "support_ratio_eval = 0.6\n",
        "\n",
        "\n",
        "#### Load data\n",
        "# Load audio files and labels\n",
        "audio_data, audio_labels = load_audio_files(data_folder, sr, duration)\n",
        "# Load eval audio files and labels\n",
        "eval_data, eval_labels = load_audio_files(eval_data_dir, sr, duration)\n",
        "\n",
        "## Labels\n",
        "labels = torch.tensor(audio_labels)\n",
        "eval_labels_tensor = torch.tensor(eval_labels)\n",
        "\n",
        "## Spectrograms\n",
        "data=generate_spectrograms(audio_data, sr, n_fft, fixed_length)\n",
        "eval_data = generate_spectrograms(eval_data, sr, n_fft, fixed_length)\n",
        "\n",
        "## Custom spectrograms\n",
        "# data=generate_custom_spectrograms(audio_data, filterbank, sr, n_fft, fixed_length)\n",
        "# eval_data = generate_custom_spectrograms(eval_data, filterbank, sr, n_fft, fixed_length)\n",
        "\n",
        "## Mel-spectrograms\n",
        "# Generate mel-spectrograms\n",
        "# data = generate_mel_spectrograms(audio_data, sr, n_fft, n_mels, n_frames)\n",
        "# eval_data = generate_mel_spectrograms(eval_data, sr, n_fft, n_mels, n_frames)\n",
        "\n",
        "## Wavelet spectrograms\n",
        "# # Generate cwt-scalograms\n",
        "# data = generate_cwt_scalograms(audio_data, sr, scales, n_frames)\n",
        "\n",
        "\n",
        "# Set variable to compare evaluation accuracies\n",
        "best_accuracy = 0.0\n",
        "best_f1_score = 0.0\n",
        "\n",
        "# Create a directory to save the best checkpoint\n",
        "checkpoint_dir = \"checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "for model_run in range(num_runs):\n",
        "    print(f\"Model run: {model_run}\")\n",
        "    #### Training Phase\n",
        "    # Instantiate the embedding model\n",
        "    embedding_model = AudioClassifier(n_bands=num_bands,\n",
        "                                      n_frames=fixed_length,\n",
        "                                      n_features=n_features)\n",
        "\n",
        "    # Set up the loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(embedding_model.parameters(),\n",
        "                          lr=learning_rate,\n",
        "                          weight_decay=weight_decay)\n",
        "    \n",
        "    # # Create the CosineAnnealingLR scheduler\n",
        "    # T_max = 35  # You can choose an appropriate value for T_max\n",
        "    # eta_min = 1e-6  # You can choose an appropriate value for eta_min\n",
        "    # scheduler = CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)\n",
        "\n",
        "\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    embedding_model = embedding_model.to(device)\n",
        "\n",
        "    # Set up interval display for high numbers of episodes\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    all_true_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    # Training loop\n",
        "    for episode in range(num_episodes):\n",
        "        # Randomly select a subset of classes\n",
        "        available_classes = torch.unique(labels)\n",
        "        selected_classes = available_classes[torch.randperm(len(available_classes))[:num_selected_classes_train]]\n",
        "        selected_classes = selected_classes.to(device)\n",
        "\n",
        "        # Initialize support and query sets\n",
        "        support_set = torch.empty((0, 1, num_bands, fixed_length))\n",
        "        support_labels = torch.empty(0, dtype=torch.long)\n",
        "        query_set = torch.empty((0, 1, num_bands, fixed_length))\n",
        "        query_labels = torch.empty(0, dtype=torch.long)\n",
        "\n",
        "        # Split the data into support and query sets\n",
        "        n_classes = len(set(audio_labels))\n",
        "\n",
        "        for class_label in selected_classes:\n",
        "            class_indices = [i for i, label in enumerate(audio_labels) if label == class_label]\n",
        "            random.shuffle(class_indices)\n",
        "            n_support = int(support_ratio_train * len(class_indices))\n",
        "            support_indices = class_indices[:n_support]\n",
        "            query_indices = class_indices[n_support:]\n",
        "\n",
        "            support_set = torch.cat((support_set, data[support_indices]), dim=0)\n",
        "            support_labels = torch.cat((support_labels, labels[support_indices]), dim=0)\n",
        "            query_set = torch.cat((query_set, data[query_indices]), dim=0)\n",
        "            query_labels = torch.cat((query_labels, labels[query_indices]), dim=0)\n",
        "\n",
        "        # Move data to available device\n",
        "        support_set = support_set.to(device)\n",
        "        support_labels = support_labels.to(device)\n",
        "        query_set = query_set.to(device)\n",
        "        query_labels = query_labels.to(device)\n",
        "\n",
        "\n",
        "        # Calculate embeddings for support and query sets\n",
        "        support_embeddings = embedding_model(support_set)\n",
        "        query_embeddings = embedding_model(query_set)\n",
        "\n",
        "        # Calculate class prototypes (mean embeddings)\n",
        "        class_prototypes = []\n",
        "        for class_label in selected_classes:\n",
        "            class_indices = (support_labels == class_label).nonzero(as_tuple=True)[0]\n",
        "            class_embeddings = support_embeddings[class_indices]\n",
        "            class_prototype = class_embeddings.mean(dim=0)\n",
        "            class_prototypes.append(class_prototype)\n",
        "        class_prototypes = torch.stack(class_prototypes)\n",
        "\n",
        "        # Calculate the distance between query samples and class prototypes\n",
        "        distances = torch.cdist(query_embeddings, class_prototypes)\n",
        "\n",
        "        # Predict the class label based on the smallest distance\n",
        "        class_probabilities = torch.softmax(-distances, dim=1)\n",
        "\n",
        "        # Remap query labels to the new range\n",
        "        query_labels_remap = torch.tensor([torch.where(selected_classes == label)[0].item() for label in query_labels], device=device)\n",
        "\n",
        "        # Calculate the loss and optimize the model\n",
        "        loss = criterion(class_probabilities, query_labels_remap)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # scheduler.step()\n",
        "\n",
        "        # Calculate accuracy and loss for the current episode\n",
        "        predictions = torch.argmax(class_probabilities, dim=1)\n",
        "        accuracy = (predictions == query_labels_remap).float().mean().item()\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy\n",
        "\n",
        "        all_true_labels.extend(query_labels_remap.cpu().numpy())\n",
        "        all_predictions.extend(predictions.cpu().numpy())\n",
        "\n",
        "        if (episode + 1) % display_interval == 0:\n",
        "            avg_loss = total_loss / display_interval\n",
        "            avg_accuracy = total_accuracy / display_interval\n",
        "            f1 = f1_score(all_true_labels, all_predictions, average='weighted')\n",
        "            \n",
        "            print(f\"Episode {episode + 1}/{num_episodes}, Avg Loss: {avg_loss:.4f}, Avg Accuracy: {avg_accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n",
        "            total_loss = 0\n",
        "            total_accuracy = 0\n",
        "            all_true_labels.clear()\n",
        "            all_predictions.clear()\n",
        "\n",
        "    #### Evaluation phase\n",
        "    embedding_model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Set up evaluation accuracy display\n",
        "    eval_total_accuracy = 0\n",
        "    eval_total_loss = 0\n",
        "    all_eval_true_labels = []\n",
        "    all_eval_predictions = []\n",
        "\n",
        "    # Evaluation loop\n",
        "    for episode in range(num_eval_episodes):\n",
        "        # Randomly select a subset of classes\n",
        "        available_classes = torch.unique(eval_labels_tensor)\n",
        "        selected_classes = available_classes[torch.randperm(len(available_classes))[:num_selected_classes_eval]]\n",
        "        selected_classes = selected_classes.to(device)\n",
        "\n",
        "        # Initialize support and query sets\n",
        "        eval_support_set = torch.empty((0, 1, num_bands, fixed_length))\n",
        "        eval_support_labels = torch.empty(0, dtype=torch.long)\n",
        "        eval_query_set = torch.empty((0, 1, num_bands, fixed_length))\n",
        "        eval_query_labels = torch.empty(0, dtype=torch.long)\n",
        "\n",
        "        # Split the data into support and query sets\n",
        "        for class_label in selected_classes:\n",
        "            class_indices = [i for i, label in enumerate(eval_labels) if label == class_label]\n",
        "            random.shuffle(class_indices)\n",
        "            n_support = int(support_ratio_eval * len(class_indices))\n",
        "            support_indices = class_indices[:n_support]\n",
        "            query_indices = class_indices[n_support:]\n",
        "\n",
        "            eval_support_set = torch.cat((eval_support_set, eval_data[support_indices]), dim=0)\n",
        "            eval_support_labels = torch.cat((eval_support_labels, eval_labels_tensor[support_indices]), dim=0)\n",
        "            eval_query_set = torch.cat((eval_query_set, eval_data[query_indices]), dim=0)\n",
        "            eval_query_labels = torch.cat((eval_query_labels, eval_labels_tensor[query_indices]), dim=0)\n",
        "\n",
        "        # Move data to available device\n",
        "        eval_support_set = eval_support_set.to(device)\n",
        "        eval_support_labels = eval_support_labels.to(device)\n",
        "        eval_query_set = eval_query_set.to(device)\n",
        "        eval_query_labels = eval_query_labels.to(device)\n",
        "\n",
        "        # Calculate embeddings for support and query sets\n",
        "        eval_support_embeddings = embedding_model(eval_support_set)\n",
        "        eval_query_embeddings = embedding_model(eval_query_set)\n",
        "\n",
        "        # Calculate class prototypes (mean embeddings)\n",
        "        class_prototypes = []\n",
        "        for class_label in selected_classes:\n",
        "            class_indices = (eval_support_labels == class_label).nonzero(as_tuple=True)[0]\n",
        "            class_embeddings = eval_support_embeddings[class_indices]\n",
        "            class_prototype = class_embeddings.mean(dim=0)\n",
        "            class_prototypes.append(class_prototype)\n",
        "        class_prototypes = torch.stack(class_prototypes)\n",
        "\n",
        "        # Calculate the distance between query samples and class prototypes\n",
        "        distances = torch.cdist(eval_query_embeddings, class_prototypes)\n",
        "\n",
        "        # Predict the class label based on the smallest distance\n",
        "        class_probabilities = torch.softmax(-distances, dim=1)\n",
        "\n",
        "        # Remap query labels to the new range\n",
        "        eval_query_labels_remap = torch.tensor([torch.where(selected_classes == label)[0].item() for label in eval_query_labels], device=device)\n",
        "\n",
        "        # Calculate the loss for the current episode\n",
        "        loss = criterion(class_probabilities, eval_query_labels_remap)\n",
        "\n",
        "        # Calculate accuracy for the current episode\n",
        "        predictions = torch.argmax(class_probabilities, dim=1)\n",
        "        accuracy = (predictions == eval_query_labels_remap).float().mean().item()\n",
        "        eval_total_loss += loss.item()\n",
        "        eval_total_accuracy += accuracy\n",
        "\n",
        "        all_eval_true_labels.extend(eval_query_labels_remap.cpu().numpy())\n",
        "        all_eval_predictions.extend(predictions.cpu().numpy())\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    eval_avg_loss = eval_total_loss / num_eval_episodes\n",
        "    eval_avg_accuracy = eval_total_accuracy / num_eval_episodes\n",
        "    f1 = f1_score(all_eval_true_labels, all_eval_predictions, average='weighted')\n",
        "\n",
        "    print(f\"Evaluation Average Loss: {eval_avg_loss:.4f}, Average Accuracy: {eval_avg_accuracy:.4f}, F1 Score: {f1:.4f} \\n\")\n",
        "\n",
        "    if f1 > best_f1_score:\n",
        "        print(f\"Previous best F1 score: {best_f1_score:.4f} - current F1 score: {f1:.4f}\")\n",
        "        best_f1_score = f1\n",
        "        best_model_weights = copy.deepcopy(embedding_model.state_dict())\n",
        "        print(\"Saving new best checkpoint... \\n\")\n",
        "        torch.save(best_model_weights, os.path.join(checkpoint_dir, 'best_checkpoint.pth'))\n",
        "\n",
        "print(f\"Evaluation complete. Saved checkpoint has F1 score of {best_f1_score:.2f}\")\n"
      ],
      "metadata": {
        "id": "jflIsQtLa2H4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f1f6703-04c2-4bfc-ea91-2d0d631018c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model run: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the model on single data samples\n",
        "Here, we visualise the individual samples tested and play the audio for demonstration.  \n",
        "The training data is used as support data with the samples tested coming from the test set."
      ],
      "metadata": {
        "id": "VBPJw7kB0BII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_single_sample(file_path, sr, duration, n_fft, n_mels, n_frames):\n",
        "    audio = load_audio_file(file_path, sr, duration)\n",
        "    mel_spectrogram = generate_mel_spectrogram(audio, sr, n_fft, n_mels, n_frames)\n",
        "    return audio, mel_spectrogram\n",
        "\n",
        "def visualize_prediction(audio, S, true_class, predicted_class):\n",
        "    fig, ax = plt.subplots(2, 1, figsize=(10, 6))\n",
        "\n",
        "    # Plot the waveform\n",
        "    ax[0].set_title(\"Waveform\")\n",
        "    librosa.display.waveshow(audio, sr=sr, ax=ax[0])\n",
        "\n",
        "    # Plot the spectrogram\n",
        "    img = librosa.display.specshow(S.squeeze().numpy(), x_axis='time', y_axis='mel', sr=sr, fmax=sr // 2, ax=ax[1])\n",
        "    ax[1].set_title(\"Mel Spectrogram\")\n",
        "    fig.colorbar(img, ax=ax[1], format=\"%+2.f dB\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    print(\"True Class Label:\", true_class)\n",
        "    print(\"Predicted Class Label:\", predicted_class)\n",
        "\n",
        "def play_audio(audio):\n",
        "    ipd.display(ipd.Audio(audio, rate=sr))\n",
        "\n",
        "test_sample_path = 'drive/MyDrive/audio_ml_data/firearm_samples/test/'\n",
        "\n",
        "# Load test sample\n",
        "test_folders = [subfolder for subfolder in os.listdir(test_sample_path) if os.path.isdir(os.path.join(test_sample_path, subfolder))]\n",
        "test_sample_folder = random.choice(test_folders)\n",
        "test_sample_path = os.path.join(test_sample_path, test_sample_folder)\n",
        "test_samples = [file for file in os.listdir(test_sample_path) if file.endswith('.wav')]\n",
        "test_sample_file = random.choice(test_samples)\n",
        "test_sample_path = os.path.join(test_sample_path, test_sample_file)\n",
        "\n",
        "audio, test_sample = load_single_sample(test_sample_path, sr, duration, n_fft, n_mels, n_frames)\n",
        "\n",
        "# Convert the test sample into a batch of size 1\n",
        "test_sample = test_sample.unsqueeze(0).to(device)\n",
        "\n",
        "# Load the best checkpoint\n",
        "embedding_model.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'best_checkpoint.pth')))\n",
        "embedding_model.eval()\n",
        "\n",
        "# Support set data\n",
        "support_data_folder = 'drive/MyDrive/audio_ml_data/firearm_samples/train/'\n",
        "\n",
        "# Load audio files and labels\n",
        "support_data, support_labels = load_audio_files(support_data_folder, sr, duration)\n",
        "\n",
        "# Generate mel-spectrograms\n",
        "support_data = generate_mel_spectrograms(support_data, sr, n_fft, n_mels, n_frames)\n",
        "support_labels_tensor = torch.tensor(support_labels)\n",
        "\n",
        "# Calculate embeddings for support and test samples\n",
        "support_embeddings = embedding_model(support_data.to(device))\n",
        "test_sample_embedding = embedding_model(test_sample)\n",
        "\n",
        "# Calculate class prototypes (mean embeddings)\n",
        "class_prototypes = []\n",
        "available_classes = torch.unique(support_labels_tensor)\n",
        "for class_label in available_classes:\n",
        "    class_indices = (support_labels_tensor == class_label).nonzero(as_tuple=True)[0]\n",
        "    class_embeddings = support_embeddings[class_indices]\n",
        "    class_prototype = class_embeddings.mean(dim=0)\n",
        "    class_prototypes.append(class_prototype)\n",
        "class_prototypes = torch.stack(class_prototypes)\n",
        "\n",
        "# Calculate the distance between the test sample and class prototypes\n",
        "distances = torch.cdist(test_sample_embedding, class_prototypes)\n",
        "\n",
        "# Predict the class label based on the smallest distance\n",
        "class_probabilities = torch.softmax(-distances, dim=1)\n",
        "prediction = torch.argmax(class_probabilities, dim=1)\n",
        "\n",
        "# Move the prediction tensor to the CPU\n",
        "prediction = prediction.cpu()\n",
        "\n",
        "# Map the predicted class label back to the original class name\n",
        "predicted_class = available_classes[prediction]\n",
        "\n",
        "print(f\"Classes: {test_folders}\")\n",
        "print(f\"Predicted class for the test sample '{test_sample_file}' is: {test_folders[predicted_class.item()]}\")\n",
        "\n",
        "# Make information available for visualisation\n",
        "true_class = test_sample_folder\n",
        "pred_class = test_folders[predicted_class.item()]\n",
        "sample = test_sample.cpu()\n",
        "\n",
        "# Visualize the prediction\n",
        "visualize_prediction(audio, sample, true_class, pred_class)\n",
        "\n",
        "# Play the audio sample\n",
        "play_audio(audio)\n"
      ],
      "metadata": {
        "id": "AU4KYO0YNu_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9WxLkTMfrEtX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}