{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1lzzOuQcsQmzXn7F6NcUOyxMk-9-Vwqoo",
      "authorship_tag": "ABX9TyN1JlQPsZ2gDAWJ2yag7Hnh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eikegermann/audio_classifier_test/blob/main/audio_classifier_cnn_2_classes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from IPython.display import Audio, display\n"
      ],
      "metadata": {
        "id": "r1Ro9HTOFH6e"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels, n_mels, max_length_s=3):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.n_mels = n_mels\n",
        "        self.max_length_s = max_length_s\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio, sample_rate = librosa.load(self.file_paths[idx], sr=44100)\n",
        "        audio = self.adjust_audio_length(audio, sample_rate)\n",
        "        mel_spectrogram = self.preprocess_data(audio, sample_rate, self.n_mels)\n",
        "        label = self.labels[idx]\n",
        "        return mel_spectrogram, label\n",
        "\n",
        "    def adjust_audio_length(self, audio, sample_rate):\n",
        "        target_length = math.ceil(self.max_length_s * sample_rate)\n",
        "        if len(audio) < target_length:\n",
        "            padding = target_length - len(audio)\n",
        "            audio = np.pad(audio, (0, padding), mode='constant')\n",
        "        elif len(audio) > target_length:\n",
        "            audio = audio[:target_length]\n",
        "        return audio\n",
        "\n",
        "    def preprocess_data(self, audio, sample_rate, n_mels):\n",
        "        hop_length = 512\n",
        "        mel_spec = librosa.feature.melspectrogram(audio,\n",
        "                                                  sr=sample_rate,\n",
        "                                                  n_fft=2048,\n",
        "                                                  hop_length=hop_length,\n",
        "                                                  n_mels=n_mels)\n",
        "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "        mel_spec = np.expand_dims(mel_spec, axis=0)  # Add channel dimension (C, H, W)\n",
        "\n",
        "        # Normalize the spectrogram\n",
        "        mean = np.mean(mel_spec)\n",
        "        std = np.std(mel_spec)\n",
        "        normalized_spec = (mel_spec - mean) / std\n",
        "\n",
        "        return normalized_spec\n"
      ],
      "metadata": {
        "id": "nQnsEp-JFLRV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioClassifier(nn.Module):\n",
        "    def __init__(self, n_mels, max_length_s):\n",
        "        super(AudioClassifier, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(p=0.4)\n",
        "\n",
        "        self.fc1 = None\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 2)\n",
        "        \n",
        "        self._initialize_fc1(n_mels, max_length_s)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "\n",
        "        return x\n",
        "    \n",
        "    def _initialize_fc1(self, n_mels, max_length_s):\n",
        "        with torch.no_grad():\n",
        "            sample_input = torch.randn(1, 1, n_mels, int(max_length_s * 44100 // 512 + 1))\n",
        "            x = self.pool(F.relu(self.bn1(self.conv1(sample_input))))\n",
        "            x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "            x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "            flattened_size = x.view(x.size(0), -1).shape[1]\n",
        "            self.fc1 = nn.Linear(flattened_size, 256)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QH2iMkr_FfVJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWM7tnhMG1Vb",
        "outputId": "83b31e3a-ea3c-4ae8-949c-1966e005a35b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and test datasets\n",
        "data_path = \"drive/MyDrive/audio_ml_data/samples/\"\n",
        "n_mels = 156\n",
        "max_length_s = 1\n",
        "test_size = 0.2\n",
        "\n",
        "\n",
        "def load_file_paths_and_labels(data_path):\n",
        "    file_paths = []\n",
        "    labels = []\n",
        "    for label, class_folder in enumerate(os.listdir(data_path)):\n",
        "        class_path = os.path.join(data_path, class_folder)\n",
        "        for audio_file in os.listdir(class_path):\n",
        "            file_path = os.path.join(class_path, audio_file)\n",
        "            file_paths.append(file_path)\n",
        "            labels.append(label)\n",
        "    return file_paths, labels\n",
        "\n",
        "\n",
        "file_paths, labels = load_file_paths_and_labels(data_path)\n",
        "\n",
        "train_file_paths, test_file_paths, train_labels, test_labels = train_test_split(\n",
        "    file_paths, labels, test_size=test_size, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "train_dataset = AudioDataset(train_file_paths, train_labels, n_mels, max_length_s)\n",
        "test_dataset = AudioDataset(test_file_paths, test_labels, n_mels, max_length_s)\n",
        "\n",
        "# Create DataLoader instances for training and testing\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Initialize the model\n",
        "model = AudioClassifier(n_mels, max_length_s)\n",
        "#model = AudioClassifier(max_length_s)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "LQ1qUxY4GQgV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "epochs = 20\n",
        "learning_rate = 3e-4\n",
        "best_test_accuracy = 0.0\n",
        "weight_decay = 0.0025\n",
        "\n",
        "# Set up the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=learning_rate,\n",
        "                       weight_decay=weight_decay)\n",
        "\n",
        "def count_samples_and_calculate_splits(data_path, test_size):\n",
        "    total_samples = 0\n",
        "    class_folders = os.listdir(data_path)\n",
        "    \n",
        "    for class_folder in class_folders:\n",
        "        class_path = os.path.join(data_path, class_folder)\n",
        "        num_samples_in_class = len(os.listdir(class_path))\n",
        "        total_samples += num_samples_in_class\n",
        "    \n",
        "    test_samples = int(total_samples * test_size)\n",
        "    train_samples = total_samples - test_samples\n",
        "    \n",
        "    return total_samples, train_samples, test_samples\n",
        "\n",
        "total_samples, train_samples, test_samples = count_samples_and_calculate_splits(data_path, test_size)\n",
        "print(f\"Total samples: {total_samples}\")\n",
        "print(f\"Training samples: {train_samples}\")\n",
        "print(f\"Test samples: {test_samples}\")\n",
        "\n",
        "def calculate_max_accuracy_change(train_samples, test_samples):\n",
        "    max_train_accuracy_change = 1 / train_samples * 100\n",
        "    max_test_accuracy_change = 1 / test_samples * 100\n",
        "    \n",
        "    return max_train_accuracy_change, max_test_accuracy_change\n",
        "\n",
        "max_train_accuracy_change, max_test_accuracy_change = calculate_max_accuracy_change(train_samples, test_samples)\n",
        "print(f\"Maximum change in training accuracy: {max_train_accuracy_change:.4f}%\")\n",
        "print(f\"Maximum change in test accuracy: {max_test_accuracy_change:.4f}%\")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for mel_spec, label in train_loader:\n",
        "        mel_spec, label = mel_spec.to(device), label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(mel_spec)\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = output.max(1)\n",
        "        total += label.size(0)\n",
        "        correct += predicted.eq(label).sum().item()\n",
        "\n",
        "    train_accuracy = 100.0 * correct / total\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss / total:.6f}, Training accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "    # Evaluation on the test set\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for mel_spec, label in test_loader:\n",
        "            mel_spec, label = mel_spec.to(device), label.to(device)\n",
        "            output = model(mel_spec)\n",
        "            loss = criterion(output, label)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = output.max(1)\n",
        "            total += label.size(0)\n",
        "            correct += predicted.eq(label).sum().item()\n",
        "\n",
        "        test_accuracy = 100.0 * correct / total\n",
        "        print(f\"Test Loss: {test_loss / total:.6f}, Test accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    # Save the best checkpoint\n",
        "    if test_accuracy > best_test_accuracy:\n",
        "        best_test_accuracy = test_accuracy\n",
        "        torch.save(model.state_dict(), \"best_checkpoint.pth\")\n",
        "        print(f\"New best checkpoint saved with accuracy: {best_test_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDm3KhJrHWLe",
        "outputId": "57e3c420-a843-43a6-f8f7-26e0649c8fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 300\n",
            "Training samples: 240\n",
            "Test samples: 60\n",
            "Maximum change in training accuracy: 0.4167%\n",
            "Maximum change in test accuracy: 1.6667%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sample(model_path, eval_data_path, n_mels=156, max_length_s=3):\n",
        "    # Load the model\n",
        "    model = AudioClassifier(n_mels, max_length_s)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    # Select a random class and sample\n",
        "    available_classes = os.listdir(eval_data_path)\n",
        "    class_folder = random.choice(available_classes)\n",
        "    class_path = os.path.join(eval_data_path, class_folder)\n",
        "    sample_path = os.path.join(class_path, random.choice(os.listdir(class_path)))\n",
        "\n",
        "    # Load the sample and create mel-spectrogram\n",
        "    audio, sample_rate = librosa.load(sample_path, sr=44100)\n",
        "    audio = adjust_audio_length(audio, sample_rate, max_length_s)\n",
        "    mel_spectrogram = preprocess_data(audio, sample_rate, n_mels)\n",
        "\n",
        "    input_tensor = torch.tensor(mel_spectrogram).unsqueeze(0)\n",
        "\n",
        "    # Make the prediction\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        prediction = torch.argmax(output, dim=1).item()\n",
        "\n",
        "\n",
        "    # Play the audio sample\n",
        "    display(Audio(audio, rate=sample_rate))\n",
        "\n",
        "    print(f\"Predicted class: {available_classes[prediction]}\")\n",
        "\n",
        "    # Display the sample waveform\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.waveshow(audio, sr=sample_rate)\n",
        "    plt.title(f\"Waveform of the audio sample (Class: {class_folder})\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.show()\n",
        "\n",
        "    # Display the mel-spectrogram\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram.squeeze(0), ref=np.max)\n",
        "    librosa.display.specshow(mel_spectrogram_db, sr=sample_rate, hop_length=512, x_axis='time', y_axis='mel')\n",
        "    plt.title(f\"Mel-spectrogram of the audio sample (Class: {class_folder})\")\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def adjust_audio_length(audio, sample_rate, max_length_s):\n",
        "    target_length = math.ceil(max_length_s * sample_rate)\n",
        "    if len(audio) < target_length:\n",
        "        padding = target_length - len(audio)\n",
        "        audio = np.pad(audio, (0, padding), mode='constant')\n",
        "    elif len(audio) > target_length:\n",
        "        audio = audio[:target_length]\n",
        "    return audio\n",
        "\n",
        "def preprocess_data(audio, sample_rate, n_mels):\n",
        "    hop_length = 512\n",
        "    mel_spec = librosa.feature.melspectrogram(audio,\n",
        "                                              sr=sample_rate,\n",
        "                                              n_fft=2048,\n",
        "                                              hop_length=hop_length,\n",
        "                                              n_mels=n_mels)\n",
        "    mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "    mel_spec = np.expand_dims(mel_spec, axis=0)  # Add channel dimension (C, H, W)\n",
        "\n",
        "    # Normalize the spectrogram\n",
        "    mean = np.mean(mel_spec)\n",
        "    std = np.std(mel_spec)\n",
        "    normalized_spec = (mel_spec - mean) / std\n",
        "\n",
        "    return normalized_spec\n",
        "\n",
        "best_checkpoint = \"best_checkpoint.pth\"\n",
        "eval_samples_path = \"drive/MyDrive/audio_ml_data/eval_samples\"\n",
        "\n"
      ],
      "metadata": {
        "id": "ZVwLWq_LNDro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sample(best_checkpoint, eval_samples_path, n_mels=n_mels, max_length_s=max_length_s)"
      ],
      "metadata": {
        "id": "jsVbqwofa-ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sample(best_checkpoint, eval_samples_path, max_length_s=max_length_s)"
      ],
      "metadata": {
        "id": "M8dbfNRCZIeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sample(best_checkpoint, eval_samples_path, max_length_s=max_length_s)"
      ],
      "metadata": {
        "id": "-tOcXVTMaxP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jflIsQtLa2H4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}